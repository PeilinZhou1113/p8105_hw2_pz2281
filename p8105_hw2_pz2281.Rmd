---
title: "p8105_hw2_pz2281"
author: Peilin Zhou
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```
## Problem 1

Read in and tidy Mr.Trash Wheel data

```{r}
trash_wheel_df = 
  readxl::read_excel("./data/Trash_Wheel.xlsx", range = "A2:N406") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(sports_balls = round(sports_balls))
```

Import and tidy precipitation data for 2018 and 2019

```{r}
pre_2018 = 
  readxl::read_excel("./data/Trash_Wheel.xlsx", sheet = 5, range = "A2:B14") %>%
  janitor::clean_names() %>% 
  mutate(year = "2018")

pre_2019 = 
  readxl::read_excel("./data/Trash_Wheel.xlsx", sheet = 4, range = "A2:B8") %>% 
  janitor::clean_names() %>% 
  mutate(year = "2019")
```

Combine data

```{r}
 tidy_pre = 
  bind_rows(pre_2018,pre_2019) %>% 
  relocate(year) %>% 
  mutate(month = month.name[month])
```



Data Summaries

The Mr.Trash Wheel dataset contains litter data for total of `r nrow(trash_wheel_df)` dumpsters through `r length(unique(trash_wheel_df$year))` years. There are `r ncol(trash_wheel_df)` variables in this dataset. For all of the dumpsters, the average number of plastic bottles they received through the five-year span is `r round(mean(trash_wheel_df$plastic_bottles))` and the average number of cigarette butts is `r round(mean(trash_wheel_df$cigarette_butts))`. The amount of total litter that that trash wheel had collected in 5 years is `r sum(trash_wheel_df$weight_tons)` tons.

The number of observation is `r nrow(pre_2018)` for the 2018 precipitation data and `r nrow(pre_2019)` for the 2019 data. Both datasets have `r ncol(pre_2018)` variables, including month, year, and precipitation amount in inches. The total amount of precipitation in 2018 was `r sum(pre_2018$total)` inches and the average amount  was `r mean(pre_2018$total)` inches. The total amount of precipitation in 2019 (data for `r nrow(pre_2019)` months) was `r sum(pre_2019$total)` inches and the median was `r median(pre_2019$total)` inches. The conmbined dataset contains precipitation data for both 2018 and 2019. The total amount of precipitation was `r sum(tidy_pre$total)` and the average was `r mean(tidy_pre$total)`.

## Problem 2

Import and tidy pols-month data

```{r, message = FALSE}
pols_month =
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
  separate(mon, into = c("year", "month","day"), sep = "-") %>% 
  mutate(month = as.numeric(month)) %>% 
  mutate(month = month.name[month]) %>% 
  mutate(president = ifelse(prez_gop == 1, "gop", "dem")) %>% 
  select(-prez_dem,-prez_gop,-day)
```

Import and tidy snp data

```{r, message = FALSE}
snp_df = 
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  mutate(date = lubridate::mdy(date)) %>% 
  separate(date, into = c("year","month", "day"), sep = "-") %>%
  mutate(month = as.numeric(month)) %>%
  mutate(month = month.name[month]) %>% 
  arrange(year,month) %>% 
  relocate(year,month) %>% 
  select(-day)
```

Import and tidy unemployment data

```{r, message = FALSE}
unemp_df = 
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment") %>% 
  mutate(month = match(month,month.abb)) %>% 
  mutate(month = month.name[month]) %>% 
  janitor::clean_names() %>% 
  mutate(year = as.character(year))
```

Merge snp_df to pols_month and then merge unemp_df into the merged data using `left_join`

```{r}
pols_snp_df = left_join(pols_month, snp_df, by = c("year", "month"))

merge_df = left_join(pols_snp_df, unemp_df, by = c("year", "month"))
```


Data Summaries

The pols_month dataset contains `r nrow(pols_month)` observations and `r ncol(pols_month)` variables. It is documenting the number of national politicians who are democratic or republican throughout `r max(as.numeric(pols_month$year)) - min(as.numeric(pols_month$year))` years. The key variables include the number of republican senators, governors, representatives, and president, as well as the number of democratic senators, governors, representatives, and president at each year and associated month. The data documented the information through `r pols_month$month[1]`, `r min(as.numeric(pols_month$year))` to `r tail(pols_month$month,1)`, `r max(as.numeric(pols_month$year))`.

The snp dataset contains `r nrow(snp_df)` observations and `r ncol(snp_df)` variables. it is related to the Standard & Poorâ€™s stock market index which is a representative measure of stock market as a whole within given dates. The year range is `r max(as.numeric(snp_df$year)) - min(as.numeric(snp_df$year))` years from `r min(as.numeric(snp_df$year))` to  `r max(as.numeric(snp_df$year))`. The key variable in this dataset is close which specifies the closing values of the S&P stock index on the associated date (year-month). The average closing value is `r mean(snp_df$close)`.

The unemployment dataset contains `r nrow(unemp_df)` observations and `r ncol(unemp_df)` variables. It is related to the unemployment rate within given dates. The year range is `r max(as.numeric(unemp_df$year)) - min(as.numeric(unemp_df$year))` from `r min(as.numeric(unemp_df$year))` to  `r max(as.numeric(unemp_df$year))`. The key variable in this dataset is unemployment which specifies the percentage of unemployment on associated data (year-month). The average unemployment rate is `r mean(unemp_df$unemployment, na.rm = TRUE)`%. The highest unemployment rate is `r max(unemp_df$unemployment, na.rm = TRUE)`% and the lowest is `r min(unemp_df$unemployment, na.rm = TRUE)`%.

The merge_df is a merged dataframe from the three datasets above (snp_df and unemp_df merged into pols_month). The keys to merge the set were year and month. It contains `r nrow(merge_df)` observations and `r ncol(merge_df)` variables, which is the conbination of the previous variables in pols_month with closing market index and unemployment rate with the associated date from the pols_month data. Since two dataframes were merged into pols_month, the range of year maintains the same as that in pols_month.

## Problem 3

read in and tidy the data. 

```{r, message = FALSE}
pop_names = 
   read_csv("./data/Popular_Baby_Names.csv") %>% 
   janitor::clean_names() %>% 
   mutate(ethnicity = gsub(" ", "_", ethnicity)) %>% 
   mutate(gender = tolower(gender), ethnicity = tolower(ethnicity), childs_first_name = tolower(childs_first_name)) %>% 
   mutate(ethnicity = recode(ethnicity, "asian_and_pacific_islander" = "asian_and_paci", "white_non_hispanic" = "white_non_hisp", "black_non_hispanic" = "black_non_hisp")) %>% 
  distinct()
```

Check out the names in ethnicity to find the redundant changes (output should reflect unique ethnicity groups after correction). Then, recode categorical values in ethnicity in the previous pipeline.
```{r}
unique(pop_names$ethnicity)
```


Now, move to create tables with conditions.

```{r}
subset_olivia = pop_names[pop_names$childs_first_name == "olivia", ] %>% 
  filter(gender == "female") %>% 
  select(year_of_birth, ethnicity, rank) %>% 
  pivot_wider(
    names_from = "year_of_birth",
    values_from = "rank"
  )
subset_olivia
```

Move to create table showing the most popular name among male children over time.

```{r}
subset_boy = pop_names[pop_names$gender == "male", ] %>% 
  filter(rank == 1) %>% 
  select(year_of_birth, ethnicity, childs_first_name) %>% 
  pivot_wider(
    names_from = "year_of_birth",
    values_from = "childs_first_name"
  )
subset_boy
```

Now move to the last part of the problem. Make a scatterplot for male, white non-hispanic children born in 2016, showing the number of children with a name (y axis) against the rank in popularity of that name (x axis).

create a subset of data for plotting

```{r}
white_nonhis_2016 = pop_names[pop_names$year_of_birth == 2016, ] %>% 
  filter(gender == "male", ethnicity == "white_non_hisp") %>% 
  select(childs_first_name, count, rank)
white_nonhis_2016
```

















